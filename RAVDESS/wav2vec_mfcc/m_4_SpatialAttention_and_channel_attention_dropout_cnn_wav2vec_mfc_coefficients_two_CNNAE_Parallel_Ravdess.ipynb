{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahra-sharifzadeh/Master_thesis/blob/main/RAVDESS/wav2vec_mfcc/m_4_SpatialAttention_and_channel_attention_dropout_cnn_wav2vec_mfc_coefficients_two_CNNAE_Parallel_Ravdess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExkoPoNs60Cu"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "ZcoBEft660Cv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, glob\n",
        "import librosa\n",
        "import librosa.display\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "\n",
        "from IPython.display import Image\n",
        "import warnings; warnings.filterwarnings('ignore') #matplot lib complains about librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXgX4hsZ60C1",
        "outputId": "5e4e1994-065c-4ecd-ef90-92cceebd6aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# needed to import dataset from google drive into colab\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T8GiBMh2CJCR"
      },
      "outputs": [],
      "source": [
        "# copy RAVDESS dataset from gdrive and unzip\n",
        "!cp '/content/gdrive/MyDrive/RAVDESS.zip' .\n",
        "!unzip -q RAVDESS.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADgTrt0X60C3"
      },
      "source": [
        "## Define features\n",
        "\n",
        "Define features as in the previous notebook on this task from my ['sklearn-audio-classification' repo](https://github.com/IliaZenkov/sklearn-audio-classification). That notebook explains the motivation behind the Mel Spectrogram and its derivative MFCC, which we use as a feature. In short, we're looking for transitions in audible pitch frequencies.\n",
        "\n",
        "**MFCCs alone provide the best accuracy in this model with training considerations in mind - and provide as good an accuracy as using chromagrams + mel spectrograms + MFCCs. We don't want extra complexity in a highly parameterized deep neural net such as this one** (unless we absolutely need it).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OBZMdi7l60C4"
      },
      "outputs": [],
      "source": [
        "# RAVDESS native sample rate is 16000\n",
        "sample_rate = 16000\n",
        "\n",
        "# Mel Spectrograms are not directly used as a feature in this model\n",
        "# Mel Spectrograms are used in calculating MFCCs, which are a higher-level representation of pitch transition\n",
        "# MFCCs work better - left the mel spectrogram function here in case anyone wants to experiment\n",
        "def feature_melspectrogram(\n",
        "    waveform,\n",
        "    sample_rate,\n",
        "    fft = 1024,\n",
        "    winlen = 512,\n",
        "    window='hamming',\n",
        "    hop=256,\n",
        "    mels=128,\n",
        "    ):\n",
        "\n",
        "    # Produce the mel spectrogram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    # Using 8khz as upper frequency bound should be enough for most speech classification tasks\n",
        "    melspectrogram = librosa.feature.melspectrogram(\n",
        "        y=waveform,\n",
        "        sr=sample_rate,\n",
        "        n_fft=fft,\n",
        "        win_length=winlen,\n",
        "        window=window,\n",
        "        hop_length=hop,\n",
        "        n_mels=mels,\n",
        "        fmax=sample_rate/2)\n",
        "\n",
        "    # convert from power (amplitude**2) to decibels\n",
        "    # necessary for network to learn - doesn't converge with raw power spectrograms\n",
        "    melspectrogram = librosa.power_to_db(melspectrogram, ref=np.max)\n",
        "\n",
        "    return melspectrogram\n",
        "\n",
        "def feature_mfcc(\n",
        "    waveform,\n",
        "    sample_rate,\n",
        "    n_mfcc = 40,\n",
        "    fft = 1024,\n",
        "    winlen = 512,\n",
        "    window='hamming',\n",
        "    #hop=256, # increases # of time steps; was not helpful\n",
        "    mels=128\n",
        "    ):\n",
        "\n",
        "    # Compute the MFCCs for all STFT frames\n",
        "    # 40 mel filterbanks (n_mfcc) = 40 coefficients\n",
        "    mfc_coefficients=librosa.feature.mfcc(\n",
        "        y=waveform,\n",
        "        sr=sample_rate,\n",
        "        n_mfcc=n_mfcc,\n",
        "        n_fft=fft,\n",
        "        win_length=winlen,\n",
        "        window=window,\n",
        "        #hop_length=hop,\n",
        "        n_mels=mels,\n",
        "        fmax=sample_rate/2\n",
        "        )\n",
        "\n",
        "    return mfc_coefficients\n",
        "\n",
        "def get_features(waveforms, features, samplerate):\n",
        "\n",
        "    # initialize counter to track progress\n",
        "    file_count = 0\n",
        "\n",
        "    # process each waveform individually to get its MFCCs\n",
        "    for waveform in waveforms:\n",
        "        mfccs = feature_mfcc(waveform, sample_rate)\n",
        "        features.append(mfccs)\n",
        "        file_count += 1\n",
        "        # print progress\n",
        "        print('\\r'+f' Processed {file_count}/{len(waveforms)} waveforms',end='')\n",
        "\n",
        "    # return all features from list of waveforms\n",
        "    return features\n",
        "\n",
        "def get_waveforms(file):\n",
        "\n",
        "    # load an individual sample audio file\n",
        "    # read the full 3 seconds of the file, cut off the first 0.5s of silence; native sample rate = 48k\n",
        "    # don't need to store the sample rate that librosa.load returns\n",
        "    waveform, _ = librosa.load(file, duration=3, offset=0.5, sr=sample_rate)\n",
        "\n",
        "    # make sure waveform vectors are homogenous by defining explicitly\n",
        "    waveform_homo = np.zeros((int(sample_rate*3,)))\n",
        "    waveform_homo[:len(waveform)] = waveform\n",
        "\n",
        "    # return a single file's waveform\n",
        "    return waveform_homo\n",
        "\n",
        "# RAVDESS dataset emotions\n",
        "# shift emotions left to be 0 indexed for PyTorch\n",
        "emotions_dict ={\n",
        "    '0':'surprised',\n",
        "    '1':'neutral',\n",
        "    '2':'calm',\n",
        "    '3':'happy',\n",
        "    '4':'sad',\n",
        "    '5':'angry',\n",
        "    '6':'fearful',\n",
        "    '7':'disgust'\n",
        "}\n",
        "\n",
        "# Additional attributes from RAVDESS to play with\n",
        "emotion_attributes = {\n",
        "    '01': 'normal',\n",
        "    '02': 'strong'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoHM-Dh860C6"
      },
      "source": [
        "## Load Data and Extract Features\n",
        "\n",
        "\n",
        "We process each file in the dataset and extract its features.\n",
        "\n",
        "We return the waveforms and the labels (from the file names of the RAVDESS audio samples). We return the raw waveforms because we're going to do some extra processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XasJEOjN60C7"
      },
      "outputs": [],
      "source": [
        "# path to data for glob\n",
        "data_path = '/content/Actor_*/*.wav'\n",
        "\n",
        "def load_data():\n",
        "    # features and labels\n",
        "    emotions = []\n",
        "    # raw waveforms to augment later\n",
        "    waveforms = []\n",
        "    # extra labels\n",
        "    intensities, genders = [],[]\n",
        "    # progress counter\n",
        "    file_count = 0\n",
        "    for file in glob.glob(data_path):\n",
        "        # get file name with labels\n",
        "        file_name = os.path.basename(file)\n",
        "\n",
        "        # get emotion label from the sample's file\n",
        "        emotion = int(file_name.split(\"-\")[2])\n",
        "\n",
        "        #  move surprise to 0 for cleaner behaviour with PyTorch/0-indexing\n",
        "        if emotion == 8: emotion = 0 # surprise is now at 0 index; other emotion indeces unchanged\n",
        "\n",
        "        # can convert emotion label to emotion string if desired, but\n",
        "        # training on number is better; better convert to emotion string after predictions are ready\n",
        "        # emotion = emotions_dict[str(emotion)]\n",
        "\n",
        "        # get other labels we might want\n",
        "        intensity = emotion_attributes[file_name.split(\"-\")[3]]\n",
        "        # even actors are female, odd are male\n",
        "        if (int((file_name.split(\"-\")[6]).split(\".\")[0]))%2==0:\n",
        "            gender = 'female'\n",
        "        else:\n",
        "            gender = 'male'\n",
        "\n",
        "        # get waveform from the sample\n",
        "        waveform = get_waveforms(file)\n",
        "\n",
        "        # store waveforms and labels\n",
        "        waveforms.append(waveform)\n",
        "        emotions.append(emotion)\n",
        "        intensities.append(intensity) # store intensity in case we wish to predict\n",
        "        genders.append(gender) # store gender in case we wish to predict\n",
        "\n",
        "        file_count += 1\n",
        "        # keep track of data loader's progress\n",
        "        print('\\r'+f' Processed {file_count}/{1440} audio samples',end='')\n",
        "\n",
        "    return waveforms, emotions, intensities, genders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37LrPb2i60C9",
        "outputId": "b15f733b-3f91-47e3-c119-ac53cec6fdac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 1440/1440 audio samples"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "# init explicitly to prevent data leakage from past sessions, since load_data() appends\n",
        "waveforms, emotions, intensities, genders = [],[],[],[]\n",
        "waveforms, emotions, intensities, genders = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLBp0B-n60DB"
      },
      "source": [
        "## Check extracted audio waveforms and labels:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjjUpfNm60DC",
        "outputId": "b0aad49b-36e0-4a9b-e94d-08da952a1681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waveforms set: 1440 samples\n",
            "Waveform signal length: 48000\n",
            "Emotions set: 1440 sample labels\n"
          ]
        }
      ],
      "source": [
        "print(f'Waveforms set: {len(waveforms)} samples')\n",
        "# we have 1440 waveforms but we need to know their length too; should be 3 sec * 48k = 144k\n",
        "print(f'Waveform signal length: {len(waveforms[0])}')\n",
        "print(f'Emotions set: {len(emotions)} sample labels')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkfe5cWQGa-P"
      },
      "source": [
        "Looks good. 1440 samples and 1440 labels in total.\n",
        "\n",
        "**Waveforms are 144k long because 3 seconds * 48k sample rate = 144k length array representing the 3 second audio snippet.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDnnCn8HEOQq"
      },
      "source": [
        "## Split into Train/Validation/Test Sets\n",
        "We'll use an 80/10/10 train/validation/test split to maximize training data and keep a reasonable validation/test set.\n",
        "\n",
        "**We're splitting waveforms so we can process train/validation/test waveforms separately and avoid data leakage.**\n",
        "\n",
        "**Have to take care to split the sets proportionally w.r.t. emotion.**\n",
        "\n",
        "**Yes, we can use sklearn - but to convince myself I've eradicated data leakage issues I did this manually.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofaAetu48YEg",
        "outputId": "05457cb2-b310-40ae-f461-8f962e451a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training waveforms:(1147, 48000), y_train:(1147,)\n",
            "Validation waveforms:(143, 48000), y_valid:(143,)\n",
            "Test waveforms:(150, 48000), y_test:(150,)\n",
            "\n",
            "Sets are unique: 1440 samples out of 1440 are unique\n"
          ]
        }
      ],
      "source": [
        "# create storage for train, validation, test sets and their indices\n",
        "train_set,valid_set,test_set = [],[],[]\n",
        "X_train,X_valid,X_test = [],[],[]\n",
        "y_train,y_valid,y_test = [],[],[]\n",
        "\n",
        "# convert waveforms to array for processing\n",
        "waveforms = np.array(waveforms)\n",
        "\n",
        "# process each emotion separately to make sure we builf balanced train/valid/test sets\n",
        "for emotion_num in range(len(emotions_dict)):\n",
        "\n",
        "    # find all indices of a single unique emotion\n",
        "    emotion_indices = [index for index, emotion in enumerate(emotions) if emotion==emotion_num]\n",
        "\n",
        "    # seed for reproducibility\n",
        "    np.random.seed(69)\n",
        "    # shuffle indicies\n",
        "    emotion_indices = np.random.permutation(emotion_indices)\n",
        "\n",
        "    # store dim (length) of the emotion list to make indices\n",
        "    dim = len(emotion_indices)\n",
        "\n",
        "    # store indices of training, validation and test sets in 80/10/10 proportion\n",
        "    # train set is first 80%\n",
        "    train_indices = emotion_indices[:int(0.8*dim)]\n",
        "    # validation set is next 10% (between 80% and 90%)\n",
        "    valid_indices = emotion_indices[int(0.8*dim):int(0.9*dim)]\n",
        "    # test set is last 10% (between 90% - end/100%)\n",
        "    test_indices = emotion_indices[int(0.9*dim):]\n",
        "\n",
        "    # create train waveforms/labels sets\n",
        "    X_train.append(waveforms[train_indices,:])\n",
        "    y_train.append(np.array([emotion_num]*len(train_indices),dtype=np.int32))\n",
        "    # create validation waveforms/labels sets\n",
        "    X_valid.append(waveforms[valid_indices,:])\n",
        "    y_valid.append(np.array([emotion_num]*len(valid_indices),dtype=np.int32))\n",
        "    # create test waveforms/labels sets\n",
        "    X_test.append(waveforms[test_indices,:])\n",
        "    y_test.append(np.array([emotion_num]*len(test_indices),dtype=np.int32))\n",
        "\n",
        "    # store indices for each emotion set to verify uniqueness between sets\n",
        "    train_set.append(train_indices)\n",
        "    valid_set.append(valid_indices)\n",
        "    test_set.append(test_indices)\n",
        "\n",
        "# concatenate, in order, all waveforms back into one array\n",
        "X_train = np.concatenate(X_train,axis=0)\n",
        "X_valid = np.concatenate(X_valid,axis=0)\n",
        "X_test = np.concatenate(X_test,axis=0)\n",
        "\n",
        "# concatenate, in order, all emotions back into one array\n",
        "y_train = np.concatenate(y_train,axis=0)\n",
        "y_valid = np.concatenate(y_valid,axis=0)\n",
        "y_test = np.concatenate(y_test,axis=0)\n",
        "\n",
        "# combine and store indices for all emotions' train, validation, test sets to verify uniqueness of sets\n",
        "train_set = np.concatenate(train_set,axis=0)\n",
        "valid_set = np.concatenate(valid_set,axis=0)\n",
        "test_set = np.concatenate(test_set,axis=0)\n",
        "\n",
        "# check shape of each set\n",
        "print(f'Training waveforms:{X_train.shape}, y_train:{y_train.shape}')\n",
        "print(f'Validation waveforms:{X_valid.shape}, y_valid:{y_valid.shape}')\n",
        "print(f'Test waveforms:{X_test.shape}, y_test:{y_test.shape}')\n",
        "\n",
        "# make sure train, validation, test sets have no overlap/are unique\n",
        "# get all unique indices across all sets and how many times each index appears (count)\n",
        "uniques, count = np.unique(np.concatenate([train_set,test_set,valid_set],axis=0), return_counts=True)\n",
        "\n",
        "# if each index appears just once, and we have 1440 such unique indices, then all sets are unique\n",
        "if sum(count==1) == len(emotions):\n",
        "    print(f'\\nSets are unique: {sum(count==1)} samples out of {len(emotions)} are unique')\n",
        "else:\n",
        "    print(f'\\nSets are NOT unique: {sum(count==1)} samples out of {len(emotions)} are unique')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kikSzy0MCFix"
      },
      "source": [
        "## Extract Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ0phzMzy40S"
      },
      "source": [
        "Extract the features from unaugmented waveforms first. In the next step, we'll append features from augmented waveforms to these 'native' features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OI4y4gFgCED3"
      },
      "outputs": [],
      "source": [
        "# # initialize feature arrays\n",
        "# # We extract MFCC features from waveforms and store in respective 'features' array\n",
        "# features_train, features_valid, features_test = [],[],[]\n",
        "\n",
        "# print('Train waveforms:') # get training set features\n",
        "# features_train = get_features(X_train, features_train, sample_rate)\n",
        "\n",
        "# print('\\n\\nValidation waveforms:') # get validation set features\n",
        "# features_valid = get_features(X_valid, features_valid, sample_rate)\n",
        "\n",
        "# print('\\n\\nTest waveforms:') # get test set features\n",
        "# features_test = get_features(X_test, features_test, sample_rate)\n",
        "\n",
        "# print(f'\\n\\nFeatures set: {len(features_train)+len(features_test)+len(features_valid)} total, {len(features_train)} train, {len(features_valid)} validation, {len(features_test)} test samples')\n",
        "# print(f'Features (MFC coefficient matrix) shape: {len(features_train[0])} mel frequency coefficients x {len(features_train[0][1])} time steps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx3PipOL60DG"
      },
      "source": [
        "## Augmenting the Data with AWGN - Additive White Gaussian Noise\n",
        "\n",
        "### Motivation\n",
        "\n",
        "Since our dataset is small, it is prone to overfitting - especially with highly parameterized deep neural net models\n",
        "such as the one we aim to build in this notebook. As such, we're going to want to augment our data. Generating more real samples will be immensely difficult. Instead, we can add white noise to the audio signals - not only to mask the effect of random noise present in the training set - but also **to create pseudo-new training samples and offset the impact of noise intrinsic to the dataset.**\n",
        "\n",
        "In addition, the RAVDESS dataset is extremely clean - we will likely want to make predictions on noisy, real-world data - yet another reason to augment the training data.\n",
        "\n",
        "We're going to use Additive White Gaussian Noise (AWGN). It's Additive because we're adding it to the source audio signal,\n",
        "**it's Gaussian because the noise vector will be sampled from a normal distribution and have a time average of zero (zero-mean), and it's white because after a whitening transformation the noise will add power to the audio signal uniformly across the frequency distribution.**\n",
        "\n",
        "We need a good balance of noise - too little will be useless, and too much will make it too difficult for the network to learn from the training data. **Note that this is just for training - we would _not_ need to add AWGN to real-world data on which we make predictions** (although we could).\n",
        "\n",
        "### Math\n",
        "The key parameters in AWGN are the signal to noise ratio (SNR), defining the magnitude of the noise added w.r.t. the audio signal. We parameterize AWGN with the minimum and maximize SNR so we can pick a random SNR to use in augmenting each sample's waveform.\n",
        "\n",
        "We need to constrain covariance to make it true AWGN. **We make a zero-mean vector of Gaussian noises (np.random.normal) that are statistically dependent. We need to apply a [whitening transformation](https://en.wikipedia.org/wiki/Whitening_transformation)**, a linear transformation taking a vector of random normal (Gaussian) variables with a known covariance matrix and mapping it to a new vector whose covariance is the identity matrix, i.e. the vector is now perfectly uncorrelated with a diaganol covariance matrix, each point of noise having variance == stdev == 1. **The whitening transformation by definition transforms a vector into a white noise vector.**\n",
        "\n",
        "We're going to add the AWGN augmented waveforms as new samples to our dataset. **Since we generate AWGN which is random for each and every sample - random random noise - we can add multiples of our noise-augmented dataset. I'll add 2 extra identical, randomly noisy datasets with 1440 samples each to get a dataset with 1440 native + 1440x2 == 4320 noisy samples.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "T6iY69E-60DG"
      },
      "outputs": [],
      "source": [
        "# def awgn_augmentation(waveform, multiples=2, bits=16, snr_min=15, snr_max=30):\n",
        "\n",
        "#     # get length of waveform (should be 3*48k = 144k)\n",
        "#     wave_len = len(waveform)\n",
        "\n",
        "#     # Generate normally distributed (Gaussian) noises\n",
        "#     # one for each waveform and multiple (i.e. wave_len*multiples noises)\n",
        "#     noise = np.random.normal(size=(multiples, wave_len))\n",
        "\n",
        "#     # Normalize waveform and noise\n",
        "#     norm_constant = 2.0**(bits-1)\n",
        "#     norm_wave = waveform / norm_constant\n",
        "#     norm_noise = noise / norm_constant\n",
        "\n",
        "#     # Compute power of waveform and power of noise\n",
        "#     signal_power = np.sum(norm_wave ** 2) / wave_len\n",
        "#     noise_power = np.sum(norm_noise ** 2, axis=1) / wave_len\n",
        "\n",
        "#     # Choose random SNR in decibels in range [15,30]\n",
        "#     snr = np.random.randint(snr_min, snr_max)\n",
        "\n",
        "#     # Apply whitening transformation: make the Gaussian noise into Gaussian white noise\n",
        "#     # Compute the covariance matrix used to whiten each noise\n",
        "#     # actual SNR = signal/noise (power)\n",
        "#     # actual noise power = 10**(-snr/10)\n",
        "#     covariance = np.sqrt((signal_power / noise_power) * 10 ** (- snr / 10))\n",
        "#     # Get covariance matrix with dim: (144000, 2) so we can transform 2 noises: dim (2, 144000)\n",
        "#     covariance = np.ones((wave_len, multiples)) * covariance\n",
        "\n",
        "#     # Since covariance and noise are arrays, * is the haddamard product\n",
        "#     # Take Haddamard product of covariance and noise to generate white noise\n",
        "#     multiple_augmented_waveforms = waveform + covariance.T * noise\n",
        "\n",
        "#     return multiple_augmented_waveforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def augment_waveforms(waveforms,  emotions, multiples):\n",
        "#     # keep track of how many waveforms we've processed so we can add correct emotion label in the same order\n",
        "#     emotion_count = 0\n",
        "#     # keep track of how many augmented samples we've added\n",
        "#     added_count = 0\n",
        "#     # convert emotion array to list for more efficient appending\n",
        "#     emotions = emotions.tolist()\n",
        "\n",
        "#     for waveform in waveforms:\n",
        "\n",
        "#         # Generate 2 augmented multiples of the dataset, i.e. 1440 native + 1440*2 noisy = 4320 samples total\n",
        "#         augmented_waveforms = awgn_augmentation(waveform, multiples=multiples)\n",
        "#         waveforms= np.concatenate([waveforms,augmented_waveforms],axis=0)\n",
        "\n",
        "\n",
        "#         # compute spectrogram for each of 2 augmented waveforms\n",
        "#         for augmented_waveform in augmented_waveforms:\n",
        "\n",
        "#             # Compute MFCCs over augmented waveforms\n",
        "#             augmented_mfcc = feature_mfcc(augmented_waveform, sample_rate=sample_rate)\n",
        "\n",
        "#             # append the augmented spectrogram to the rest of the native data\n",
        "#             # features.append(augmented_mfcc)\n",
        "#             # print(augmented_waveforms.shape)\n",
        "#             emotions.append(emotions[emotion_count])\n",
        "\n",
        "#             # keep track of new augmented samples\n",
        "#             added_count += 1\n",
        "\n",
        "#             # check progress\n",
        "#             print('\\r'+f'Processed {emotion_count + 1}/{len(waveforms)} waveforms for {added_count}/{len(waveforms)*multiples} new augmented samples',end='')\n",
        "\n",
        "#         # keep track of the emotion labels to append in order\n",
        "#         emotion_count += 1\n",
        "\n",
        "#         # store augmented waveforms to check their shape\n",
        "#         augmented_waveforms_temp.append(augmented_waveforms)\n",
        "\n",
        "\n",
        "#     return waveforms, augmented_mfcc,  emotions"
      ],
      "metadata": {
        "id": "61KDvjLQngzZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GZ2CAy3F-nK"
      },
      "source": [
        "### Compute AWGN-augmented features and add to the rest of the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def addAWGN(signal, num_bits=16, augmented_num=4, snr_low=15, snr_high=30):\n",
        "    signal_len = len(signal)\n",
        "    # Generate White Gaussian noise\n",
        "    noise = np.random.normal(size=(augmented_num, signal_len))\n",
        "    # Normalize signal and noise\n",
        "    norm_constant = 2.0**(num_bits-1)\n",
        "    signal_norm = signal / norm_constant\n",
        "    noise_norm = noise / norm_constant\n",
        "    # Compute signal and noise power\n",
        "    s_power = np.sum(signal_norm ** 2) / signal_len\n",
        "    n_power = np.sum(noise_norm ** 2, axis=1) / signal_len\n",
        "    # Random SNR: Uniform [15, 30] in dB\n",
        "    target_snr = np.random.randint(snr_low, snr_high)\n",
        "    # Compute K (covariance matrix) for each noise\n",
        "    K = np.sqrt((s_power / n_power) * 10 ** (- target_snr / 10))\n",
        "    K = np.ones((signal_len, augmented_num)) * K\n",
        "    # Generate noisy signal\n",
        "    return signal + K.T * noise"
      ],
      "metadata": {
        "id": "g_EwfKc3j7wR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_signals = []\n",
        "aug_labels = []\n",
        "emotion_list = y_train.tolist()\n",
        "for i in range(X_train.shape[0]):\n",
        "    signal = X_train[i,:]\n",
        "    augmented_signals = addAWGN(signal)\n",
        "    for j in range(augmented_signals.shape[0]):\n",
        "        aug_labels.append(emotion_list[i])\n",
        "        aug_signals.append(augmented_signals[j,:])\n",
        "        # data = data.append(data.iloc[i], ignore_index=True)\n",
        "    print(\"\\r Processed {}/{} files\".format(i+1,X_train.shape[0]),end='')\n",
        "aug_signals = np.stack(aug_signals,axis=0)\n",
        "X_train = np.concatenate([X_train,aug_signals],axis=0)\n",
        "aug_labels = np.stack(aug_labels,axis=0)\n",
        "y_train = np.concatenate([y_train,aug_labels])\n",
        "print('')\n",
        "print(f'X_train:{X_train.shape}, Y_train:{y_train.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1J6XnXOkAgZ",
        "outputId": "a60b6f52-04c5-46b4-b1e1-34c10c642d78"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 1147/1147 files\n",
            "X_train:(5735, 48000), Y_train:(5735,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug_signals = []\n",
        "aug_labels = []\n",
        "emotion_list = y_valid.tolist()\n",
        "for i in range(X_valid.shape[0]):\n",
        "    signal = X_valid[i,:]\n",
        "    augmented_signals = addAWGN(signal)\n",
        "    for j in range(augmented_signals.shape[0]):\n",
        "        aug_labels.append(emotion_list[i])\n",
        "        aug_signals.append(augmented_signals[j,:])\n",
        "        # data = data.append(data.iloc[i], ignore_index=True)\n",
        "    print(\"\\r Processed {}/{} files\".format(i+1,y_valid.shape[0]),end='')\n",
        "aug_signals = np.stack(aug_signals,axis=0)\n",
        "X_valid = np.concatenate([X_valid,aug_signals],axis=0)\n",
        "aug_labels = np.stack(aug_labels,axis=0)\n",
        "y_valid = np.concatenate([y_valid,aug_labels])\n",
        "print('')\n",
        "print(f'X_valid:{X_valid.shape}, Y_train:{y_valid.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3DO8dkZoDtb",
        "outputId": "717cfa31-e633-4ce9-cb31-272fe746b5b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 143/143 files\n",
            "X_valid:(715, 48000), Y_train:(715,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aug_signals = []\n",
        "aug_labels = []\n",
        "emotion_list = y_test.tolist()\n",
        "for i in range(X_test.shape[0]):\n",
        "    signal = X_test[i,:]\n",
        "    augmented_signals = addAWGN(signal)\n",
        "    for j in range(augmented_signals.shape[0]):\n",
        "        aug_labels.append(emotion_list[i])\n",
        "        aug_signals.append(augmented_signals[j,:])\n",
        "        # data = data.append(data.iloc[i], ignore_index=True)\n",
        "    print(\"\\r Processed {}/{} files\".format(i+1,y_test.shape[0]),end='')\n",
        "aug_signals = np.stack(aug_signals,axis=0)\n",
        "X_test = np.concatenate([X_test,aug_signals],axis=0)\n",
        "aug_labels = np.stack(aug_labels,axis=0)\n",
        "y_test = np.concatenate([y_test,aug_labels])\n",
        "print('')\n",
        "print(f'X_valid:{X_test.shape}, Y_train:{y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeBXMOwjotEx",
        "outputId": "de724d91-282d-4b30-8ee9-7729a958c7e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 150/150 files\n",
            "X_valid:(750, 48000), Y_train:(750,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize feature arrays\n",
        "# We extract MFCC features from waveforms and store in respective 'features' array\n",
        "features_train, features_valid, features_test = [],[],[]\n",
        "\n",
        "print('Train waveforms:') # get training set features\n",
        "features_train = get_features(X_train, features_train, sample_rate)\n",
        "\n",
        "print('\\n\\nValidation waveforms:') # get validation set features\n",
        "features_valid = get_features(X_valid, features_valid, sample_rate)\n",
        "\n",
        "print('\\n\\nTest waveforms:') # get test set features\n",
        "features_test = get_features(X_test, features_test, sample_rate)\n",
        "\n",
        "print(f'\\n\\nFeatures set: {len(features_train)+len(features_test)+len(features_valid)} total, {len(features_train)} train, {len(features_valid)} validation, {len(features_test)} test samples')\n",
        "print(f'Features (MFC coefficient matrix) shape: {len(features_train[0])} mel frequency coefficients x {len(features_train[0][1])} time steps')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soHKB0MppPos",
        "outputId": "e57082c5-2412-4b44-a554-219f49057923"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train waveforms:\n",
            " Processed 5735/5735 waveforms\n",
            "\n",
            "Validation waveforms:\n",
            " Processed 715/715 waveforms\n",
            "\n",
            "Test waveforms:\n",
            " Processed 750/750 waveforms\n",
            "\n",
            "Features set: 7200 total, 5735 train, 715 validation, 750 test samples\n",
            "Features (MFC coefficient matrix) shape: 40 mel frequency coefficients x 94 time steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "05bOxnra54ia"
      },
      "outputs": [],
      "source": [
        "# # # store augmented waveforms to verify their shape and random-ness\n",
        "# augmented_waveforms_temp = []\n",
        "\n",
        "# # # specify multiples of our dataset to add as augmented data\n",
        "# multiples = 2\n",
        "\n",
        "# # print('Train waveforms:') # augment waveforms of training set\n",
        "# # signal, augmented_mfcc,  emotions\n",
        "# signal, features_train , y_train = augment_waveforms(X_train,  y_train, multiples)\n",
        "\n",
        "# # print('\\n\\nValidation waveforms:') # augment waveforms of validation set\n",
        "# # features_valid, y_valid = augment_waveforms(X_valid,  y_valid, multiples)\n",
        "\n",
        "# # print('\\n\\nTest waveforms:') # augment waveforms of test set\n",
        "# # features_test, y_test = augment_waveforms(X_test,  y_test, multiples)\n",
        "\n",
        "# # # Check new shape of extracted features and data:\n",
        "# # print(f'\\n\\nNative + Augmented Features set: {len(features_train)+len(features_test)+len(features_valid)} total, {len(features_train)} train, {len(features_valid)} validation, {len(features_test)} test samples')\n",
        "# # print(f'{len(y_train)} training sample labels, {len(y_valid)} validation sample labels, {len(y_test)} test sample labels')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG6UTrZ160DO"
      },
      "source": [
        "### Check Augmented Waveforms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "co938icT60DP"
      },
      "outputs": [],
      "source": [
        "# # pick a random waveform, but same one from native and augmented set for easier comparison\n",
        "# plt.figure(figsize=(15,4))\n",
        "# plt.subplot(1, 2, 1)\n",
        "# librosa.display.waveshow(waveforms[12], sr=sample_rate)\n",
        "# plt.title('Native')\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# # augmented waveforms are 2D len 1440 list with 2 waveforms in each position\n",
        "# librosa.display.waveshow(augmented_waveforms_temp[0][0], sr=sample_rate)\n",
        "# plt.title('AWGN Augmented')\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(figsize=(15,4))\n",
        "# plt.subplot(1, 2, 1)\n",
        "# librosa.display.waveshow(augmented_waveforms_temp[2][0], sr=sample_rate)\n",
        "# plt.title('AWGN Augmented')\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# librosa.display.waveshow(augmented_waveforms_temp[7][0], sr=sample_rate)\n",
        "# plt.title('AWGN Augmented')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-GOeKiP60DV"
      },
      "source": [
        "Looks noisy alright. Noise is clearly visible in otherwise-silent regions of the waveform. We can see the variability of the noise, which should have an SNR between 15 and 30.\n",
        "\n",
        "**Note that augmentation was only done after splitting data into train, validation, and test sets - and we processed each set separately.**\n",
        "\n",
        "**When we augmented the data before splitting it, test and validation data leaked into the training set giving a 97% test accuracy after training.**\n",
        "\n",
        "## Format Data into Tensor Ready 4D Arrays\n",
        "We don't have a colour channel in our MFCC feature array of dim (#samples, #MFC coefficients, time steps). **We have an analog of a black and white image: instead of 3 colour channels, we have 1 signal intensity channel: magnitude of each of 40 mel frequency coefficients at time t.**\n",
        "\n",
        "**We need an input channel dim to expand to output channels using CNN filters. We create a dummy channel dim to expand features into 2D-CNN-ready 4D tensor format: N x C x H x W.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PQCm9rLx60DW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2461e31-fafa-4c2e-f493-b5bf1fdd20ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of 4D feature array for input tensor: (5735, 1, 40, 94) train, (715, 1, 40, 94) validation, (750, 1, 40, 94) test\n",
            "Shape of emotion labels: (5735,) train, (715,) validation, (750,) test\n"
          ]
        }
      ],
      "source": [
        "# need to make dummy input channel for CNN input feature tensor\n",
        "features_train = np.expand_dims(features_train,1)\n",
        "features_valid = np.expand_dims(features_valid, 1)\n",
        "features_test = np.expand_dims(features_test,1)\n",
        "\n",
        "# # convert emotion labels from list back to numpy arrays for PyTorch to work with\n",
        "y_train = np.array(y_train)\n",
        "y_valid = np.array(y_valid)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# confiorm that we have tensor-ready 4D data array\n",
        "# should print (batch, channel, width, height) == (4320, 1, 128, 282) when multiples==2\n",
        "print(f'Shape of 4D feature array for input tensor: {features_train.shape} train, {features_valid.shape} validation, {features_test.shape} test')\n",
        "print(f'Shape of emotion labels: {y_train.shape} train, {y_valid.shape} validation, {y_test.shape} test')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDHxL8j11OjH",
        "outputId": "2ff839a7-fe33-41a9-9c13-65e0dc1efeca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5735, 48000)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3mHDd2jiB9o6"
      },
      "outputs": [],
      "source": [
        "# free up some RAM - no longer need full feature set or any waveforms\n",
        "# del features_train, features_valid, features_test, waveforms, augmented_waveforms_temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mr8NcUd60Dd"
      },
      "source": [
        "## Feature Scaling\n",
        "Scaling will drastically decrease the length of time the model needs to train to convergence - it will have easier computations to perform on smaller magnitudes. **For reference, scaling reduces the time to convergence from about 500 to 200 epochs for this model.**\n",
        "\n",
        "**Standard Scaling makes the most sense because we have features whose target distribution we don't know.** When I performed classification on this dataset with an MLP classifier standard scaling was best across a variety of conditions and features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tlOPCDUx60De",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548b0bd9-6c3e-451b-ca74-38c388d8dd07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train scaled:(5735, 1, 40, 94), y_train:(5735,)\n",
            "X_valid scaled:(715, 1, 40, 94), y_valid:(715,)\n",
            "X_test scaled:(750, 1, 40, 94), y_test:(750,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "#### Scale the training data ####\n",
        "# store shape so we can transform it back\n",
        "N,C,H,W = features_train.shape\n",
        "# Reshape to 1D because StandardScaler operates on a 1D array\n",
        "# tell numpy to infer shape of 1D array with '-1' argument\n",
        "features_train = np.reshape(features_train, (N,-1))\n",
        "features_train = scaler.fit_transform(features_train)\n",
        "# Transform back to NxCxHxW 4D tensor format\n",
        "features_train = np.reshape(features_train, (N,C,H,W))\n",
        "\n",
        "##### Scale the validation set ####\n",
        "N,C,H,W = features_valid.shape\n",
        "features_valid = np.reshape(features_valid, (N,-1))\n",
        "features_valid = scaler.transform(features_valid)\n",
        "features_valid = np.reshape(features_valid, (N,C,H,W))\n",
        "\n",
        "#### Scale the test set ####\n",
        "N,C,H,W = features_test.shape\n",
        "features_test = np.reshape(features_test, (N,-1))\n",
        "features_test = scaler.transform(features_test)\n",
        "features_test = np.reshape(features_test, (N,C,H,W))\n",
        "\n",
        "# check shape of each set again\n",
        "print(f'X_train scaled:{features_train.shape}, y_train:{y_train.shape}')\n",
        "print(f'X_valid scaled:{features_valid.shape}, y_valid:{y_valid.shape}')\n",
        "print(f'X_test scaled:{features_test.shape}, y_test:{y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvZh_Cuv7zAb"
      },
      "source": [
        "## Save and Reload Data as NumPy Arrays\n",
        "We can save the training/validation/test data as numpy arrays to enable faster loading in case the notebook kernel crashes / google colab runtime crashes / any number of reasons the training data might be cleared from memory. This is much faster than loading 1440 files and computing their features again - not to mention augmented features."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q2EmDJEB42ty"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yx4vK5ssRIzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a0082c-b293-4ed6-ba98-4a15ddaade00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features and labels saved to features+labels.npy\n"
          ]
        }
      ],
      "source": [
        "###### SAVE #########\n",
        "# choose save file name\n",
        "filename = 'features+labels.npy'\n",
        "\n",
        "# open file in write mode and write data\n",
        "with open(filename, 'wb') as f:\n",
        "    np.save(f, features_train)\n",
        "    np.save(f, features_valid)\n",
        "    np.save(f, features_test)\n",
        "    np.save(f, y_train)\n",
        "    np.save(f, y_valid)\n",
        "    np.save(f, y_test)\n",
        "\n",
        "print(f'Features and labels saved to {filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bj46pQstRKlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e52dfba-e29c-4b33-838b-02ba0b4eedc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train:(5735, 1, 40, 94), y_train:(5735,)\n",
            "X_valid:(715, 1, 40, 94), y_valid:(715,)\n",
            "X_test:(750, 1, 40, 94), y_test:(750,)\n"
          ]
        }
      ],
      "source": [
        "##### LOAD #########\n",
        "# choose load file name\n",
        "filename = 'features+labels.npy'\n",
        "\n",
        "# open file in read mode and read data\n",
        "with open(filename, 'rb') as f:\n",
        "    features_train = np.load(f)\n",
        "    features_valid = np.load(f)\n",
        "    features_test = np.load(f)\n",
        "    y_train = np.load(f)\n",
        "    y_valid = np.load(f)\n",
        "    y_test = np.load(f)\n",
        "\n",
        "# Check that we've recovered the right data\n",
        "print(f'X_train:{features_train.shape}, y_train:{y_train.shape}')\n",
        "print(f'X_valid:{features_valid.shape}, y_valid:{y_valid.shape}')\n",
        "print(f'X_test:{features_test.shape}, y_test:{y_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "ZcdVeQiA_xLw",
        "outputId": "c7c2ed4c-30fb-41c6-cbf8-37c9366bc143",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5735, 48000)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L6ObDdbgjQaJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels * 2, 1, kernel_size=1)  # Double the channels in the convolution\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Assuming x has shape [batches, 64, 1, 8]\n",
        "        avg_pool = F.avg_pool2d(x, kernel_size=(1, x.size(3)))\n",
        "        max_pool = F.max_pool2d(x, kernel_size=(1, x.size(3)))\n",
        "\n",
        "        # Double the channels in the features concatenation\n",
        "        features = torch.cat([avg_pool, max_pool], dim=1)\n",
        "\n",
        "        attention = self.conv1(features)\n",
        "        attention = self.sigmoid(attention)\n",
        "\n",
        "        # Applying attention to the input tensor along the spatial dimension\n",
        "        x = x * attention\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "batches, channels, height, width = 2, 64, 1, 2\n",
        "input_tensor = torch.randn(batches, channels, height, width)\n",
        "attention_module = SpatialAttention(in_channels=channels)\n",
        "output_tensor = attention_module(input_tensor)\n",
        "\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "print(\"Output shape:\", output_tensor.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw7cZN8q877f",
        "outputId": "6cc4f2a6-90bf-4411-99ca-39bfc5d52ace"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 64, 1, 2])\n",
            "Output shape: torch.Size([2, 64, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels // reduction),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_channels // reduction, in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_pool = self.avg_pool(x).view(x.size(0), -1)\n",
        "        max_pool = self.max_pool(x).view(x.size(0), -1)\n",
        "        channel_att = self.fc(avg_pool + max_pool).view(x.size(0), x.size(1), 1, 1)\n",
        "        x = x * channel_att\n",
        "        return x\n",
        "\n",
        "\n",
        "# Example usage\n",
        "batches, channels, height, width = 2, 64, 1, 2\n",
        "input_tensor = torch.randn(batches, channels, height, width)\n",
        "attention_module = ChannelAttention(in_channels=channels)\n",
        "output_tensor = attention_module(input_tensor)\n",
        "\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "print(\"Output shape:\", output_tensor.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0Eq8DIg9JTR",
        "outputId": "61258d27-8bff-489c-ddde-e9a0be88a814"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 64, 1, 2])\n",
            "Output shape: torch.Size([2, 64, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder1, self).__init__()\n",
        "\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            # 1st 2D convolution layer\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.SELU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.3),\n",
        "\n",
        "            # 2nd 2D convolution layer\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.SELU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
        "            nn.Dropout(p=0.3),\n",
        "\n",
        "            # 3rd 2D convolution layer\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.SELU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder1(x)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UZfXC9Zebrf_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder1, self).__init__()\n",
        "\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            # 1st 2D transpose convolution layer\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=5, stride=(1, 4), padding=0, output_padding=(0, 2)),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.SELU(),\n",
        "\n",
        "            # 2nd 2D transpose convolution layer\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=4, padding=0, output_padding=(1, 2)),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.SELU(),\n",
        "\n",
        "            # 3rd 2D transpose convolution layer\n",
        "            nn.ConvTranspose2d(16, 1, kernel_size=(3,7), stride=2, padding=1, output_padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder1(x)\n"
      ],
      "metadata": {
        "id": "8O8F62-AU8RZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Autoencoder1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder1, self).__init__()\n",
        "        self.encoder = Encoder1()\n",
        "        self.spatial_attention = SpatialAttention(64)\n",
        "        self.channel_attention = ChannelAttention(64)\n",
        "        self.decoder = Decoder1()\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        # print(\"encoded1:\", encoded.shape)\n",
        "        sa = self.spatial_attention(encoded)\n",
        "        # print(\"sa1:\",sa.shape)\n",
        "        ca = self.channel_attention(encoded)\n",
        "        # print(\"ca1:\", ca.shape)\n",
        "        att_encoded = sa + ca + encoded\n",
        "\n",
        "        decoded = self.decoder(att_encoded)\n",
        "        return encoded, decoded\n"
      ],
      "metadata": {
        "id": "vuaEWdW7U-Hn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "autoencoder = Autoencoder1()\n",
        "input_data = torch.randn(1, 1, 40, 94)  # Replace with your input shape\n",
        "encoded1, decoded1 = autoencoder(input_data)\n",
        "encoded1.shape, decoded1.shape"
      ],
      "metadata": {
        "id": "xIiTmfm8VBYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f4d0fc-f846-4715-fab4-51ee098b43da"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 64, 1, 2]), torch.Size([1, 1, 40, 94]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Encoder2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder2, self).__init__()\n",
        "\n",
        "        # Define the 2D convolutional layers\n",
        "        self.conv2Dblock = nn.Sequential(\n",
        "            # 1st 2D convolution layer\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.SELU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.3),\n",
        "\n",
        "            # 2nd 2D convolution layer\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.SELU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
        "            nn.Dropout(p=0.3),\n",
        "\n",
        "            # 3rd 2D convolution layer\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.SELU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
        "            nn.Dropout(p=0.3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the convolutional layers\n",
        "        return self.conv2Dblock(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "vYH4SzyKbXkw"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Decoder2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder2, self).__init__()\n",
        "\n",
        "        # Define the 2D transposed convolutional layers for the decoder\n",
        "        self.decoder2 = nn.Sequential(\n",
        "            # 1st 2D transposed convolution layer\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=5, stride=(1, 4), padding=0, output_padding=(0, 2)),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.SELU(),\n",
        "\n",
        "            # 2nd 2D transposed convolution layer\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=4, padding=0, output_padding=(1, 2)),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.SELU(),\n",
        "\n",
        "            # 3rd 2D transposed convolution layer\n",
        "            nn.ConvTranspose2d(16, 1, kernel_size=(3,7), stride=2, padding=1, output_padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the decoder's transposed convolutional layers\n",
        "        return self.decoder2(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "TtOYsGn2bXYU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Autoencoder2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder2, self).__init__()\n",
        "\n",
        "        # Define the encoder and decoder\n",
        "        self.encoder = Encoder2()\n",
        "        self.spatial_attention = SpatialAttention(64)\n",
        "        self.channel_attention = ChannelAttention(64)\n",
        "        self.decoder = Decoder2()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode the input\n",
        "        encoded = self.encoder(x)\n",
        "        sa = self.spatial_attention(encoded)\n",
        "        ca = self.channel_attention(encoded)\n",
        "        att_encoded = sa + ca + encoded\n",
        "\n",
        "        # Decode the encoded representation\n",
        "        decoded = self.decoder(att_encoded)\n",
        "        return encoded, decoded\n",
        "# Example usage\n",
        "autoencoder2 = Autoencoder2()\n",
        "input_data = torch.randn(1, 1, 40, 94)  # Replace with your input shape\n",
        "encoded2, decoded2 = autoencoder2(input_data)\n",
        "encoded2.shape, decoded2.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8S9c0vKbXP7",
        "outputId": "e2636418-eb57-477f-f74d-9219e8dec7dc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 64, 1, 2]), torch.Size([1, 1, 40, 94]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        # Maxpool the input feature map/tensor to the transformer\n",
        "        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1, 4], stride=[1, 4])\n",
        "\n",
        "        # Define a single transformer encoder layer\n",
        "        transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=40,            # Input feature (frequency) dim after max-pooling: 40*282 -> 40*70 (MFC * time)\n",
        "            nhead=4,               # Number of self-attention layers in each multi-head self-attention layer in each encoder block\n",
        "            dim_feedforward=512,   # Dimension for the feedforward network in each encoder block: 40 -> 512 -> 40\n",
        "            dropout=0.4,\n",
        "            activation='relu'      # Activation function (e.g., ReLU) for feedforward network\n",
        "        )\n",
        "\n",
        "        # The complete transformer block contains 4 full transformer encoder layers\n",
        "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "                ########## 4-encoder-layer Transformer block w/ 40-->512-->40 feedfwd network ##############\n",
        "        # maxpool input feature map: 1*40*282 w/ 1*4 kernel --> 1*40*70\n",
        "        x_maxpool = self.transformer_maxpool(x)\n",
        "\n",
        "        # remove channel dim: 1*40*70 --> 40*70\n",
        "        x_maxpool_reduced = torch.squeeze(x_maxpool,1)\n",
        "\n",
        "        # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format\n",
        "        # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)\n",
        "        x = x_maxpool_reduced.permute(2,0,1)\n",
        "\n",
        "        # finally, pass reduced input feature map x into transformer encoder layers\n",
        "        transformer_output = self.transformer_encoder(x)\n",
        "\n",
        "        # create final feature emedding from transformer layer by taking mean in the time dimension (now the 0th dim)\n",
        "        # transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average\n",
        "        transformer_embedding = torch.mean(transformer_output, dim=0) # dim 40x70 --> 40\n",
        "        return transformer_embedding\n",
        "\n",
        "# You can use this TransformerBlock within your larger model as needed.\n"
      ],
      "metadata": {
        "id": "tdqK5CyM4H_n"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
        "except:\n",
        "  !pip install transformers\n",
        "  from transformers import Wav2Vec2Model, Wav2Vec2Processor\n"
      ],
      "metadata": {
        "id": "PpsDKlWozqZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e5014d-b20a-4c2f-d8df-24fc5409e83c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.19.1-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.1/311.1 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TVnM56Tz1Wi",
        "outputId": "4675b6cf-7d8f-4dc9-94f9-44ba66c405b6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5735, 48000)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, num_emotions):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "\n",
        "        # Linear softmax layer to take the final concatenated embedding tensor\n",
        "        # from parallel 2D convolutional and transformer blocks, output 8 logits\n",
        "        # Each full convolution block outputs (64*1*8) embedding flattened to dim 512 1D array\n",
        "        # Full transformer block outputs 40*70 feature map, which we time-avg to dim 40 1D array\n",
        "        # 512*2 + 40 == 1064 input features --> 8 output emotions\n",
        "        self.fc1_linear = nn.Linear(128*3 , 128)\n",
        "        self.fc2_linear = nn.Linear(128 , num_emotions)\n",
        "        self.drop = nn.Dropout(p=0.1)\n",
        "\n",
        "\n",
        "\n",
        "        # Softmax layer for the 8 output logits from the final FC linear layer\n",
        "        self.softmax_out = nn.Softmax(dim=1)  # dim==1 refers to the frequency embedding\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # Pass the concatenated features through the final linear layer\n",
        "        out1 = self.fc1_linear(x)\n",
        "        out1 = self.drop(out1)\n",
        "        output = self.fc2_linear(out1)\n",
        "\n",
        "\n",
        "        # Apply softmax to get the class probabilities\n",
        "        class_probs = self.softmax_out(output)\n",
        "\n",
        "        return output , class_probs\n",
        "\n",
        "# You can use this EmotionClassifier as part of your larger model to perform emotion classification.\n"
      ],
      "metadata": {
        "id": "3DfsBLsR463y"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNNLinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNLinearModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 1, kernel_size=(4, 4), stride=(2, 2))\n",
        "        self.drop = nn.Dropout(p=0.1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.conv2 = nn.Conv2d(1, 1, kernel_size=(4, 4), stride=(2, 2))\n",
        "        self.fc1 = nn.Linear(376, 128)  # Adjust the input size based on your input shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.drop(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        # print(x.shape)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "batches, channels, height, width = 2, 1, 149, 768\n",
        "input_tensor = torch.randn(batches, channels, height, width)\n",
        "model = CNNLinearModel()\n",
        "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
        "output_tensor = model(input_tensor)\n",
        "\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "print(\"Output shape:\", output_tensor.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMXQKZMEnkmg",
        "outputId": "f89e23cc-24d3-45d9-c203-e64fbe2da624"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable params:  48290\n",
            "Input shape: torch.Size([2, 1, 149, 768])\n",
            "Output shape: torch.Size([2, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the models\n",
        "\n",
        "\n",
        "\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self,num_emotions):\n",
        "\n",
        "        super(MultiTaskModel, self).__init__()\n",
        "        self.autoencoder1 = Autoencoder1()\n",
        "        self.autoencoder2 = Autoencoder2()\n",
        "        # self.transformerblock = TransformerBlock()\n",
        "        self.emotion_classifier = EmotionClassifier(num_emotions)\n",
        "        # self.w2c_linear1 = nn.Linear(149* 768 , 40)\n",
        "        # self.w2c_linear2 = nn.Linear(128 , 40)\n",
        "        self.reduce_w2v_dim = CNNLinearModel()\n",
        "        self.drop = nn.Dropout(p=0.1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        w2v_features= y.reshape(-1,1,y.shape[1], y.shape[2])\n",
        "        # print(w2v_features.shape)\n",
        "\n",
        "        # print(\"w2v_features:\", w2v_features.shape)\n",
        "\n",
        "        w2v_fc1_embedding = self.reduce_w2v_dim(w2v_features)\n",
        "        # print(\"w2v_fc1_embedding:\", w2v_fc1_embedding.shape)\n",
        "\n",
        "        # w2v_fc2_embedding = torch.flatten(w2v_fc1_embedding, start_dim=0)\n",
        "        # print(\"w2v_fc2_embedding:\", w2v_fc2_embedding.shape)\n",
        "\n",
        "\n",
        "\n",
        "        encoded1, decoded1 = self.autoencoder1(x)\n",
        "        encoded1_embedding1 = torch.flatten(encoded1, start_dim=1)\n",
        "        # print(\"encoded1_embedding1\", encoded1_embedding1.shape)\n",
        "        encoded2, decoded2= self.autoencoder2(x)\n",
        "        encoded2_embedding2 = torch.flatten(encoded2, start_dim=1)\n",
        "        # print(\"encoded1_embedding2\", encoded2_embedding2.shape)\n",
        "\n",
        "\n",
        "        # transformer_embedding = self.transformerblock(x)\n",
        "\n",
        "        complete_embedding = torch.cat([encoded1_embedding1, encoded2_embedding2,w2v_fc1_embedding], dim=1)\n",
        "        complete_embedding = self.drop(complete_embedding)\n",
        "\n",
        "        # print(\"complete_embedding\", complete_embedding.shape)\n",
        "\n",
        "\n",
        "        output_logits, output_softmax = self.emotion_classifier(complete_embedding)\n",
        "\n",
        "        return decoded1, decoded2, output_logits, output_softmax"
      ],
      "metadata": {
        "id": "TdjcC_6p_j1T"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "model = MultiTaskModel(num_emotions=8)\n",
        "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
        "\n",
        "input_data = torch.randn(2, 1, 40, 94)  # Replace with your input shape\n",
        "input_data2 = torch.randn( 2, 149, 768)  # Replace with your input shape\n",
        "# print(input_data2.unsqueeze(dim=1).shape)\n",
        "# rrr= input_data2.unsqueeze(dim=1)\n",
        "decoded1, decoded2, output_logits, output_softmax = model(input_data,input_data2)\n",
        "decoded1.shape, decoded2.shape, output_logits, output_softmax\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3y4K40_a-ZT",
        "outputId": "72f3b122-c718-4201-a453-b1f0f6ad6c9b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable params:  259638\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 1, 40, 94]),\n",
              " torch.Size([2, 1, 40, 94]),\n",
              " tensor([[-0.5659,  0.5908, -0.6924, -0.7227, -0.0211,  0.2390, -0.1818, -0.0181],\n",
              "         [ 0.0394,  0.5098, -0.2640, -0.1337,  0.5579,  0.7371, -0.8031, -0.0560]],\n",
              "        grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.0765, 0.2432, 0.0674, 0.0654, 0.1319, 0.1711, 0.1123, 0.1323],\n",
              "         [0.1086, 0.1738, 0.0802, 0.0913, 0.1824, 0.2182, 0.0468, 0.0987]],\n",
              "        grad_fn=<SoftmaxBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions\n",
        "autoencoder_criterion = nn.MSELoss()  # Use Mean Squared Error for autoencoder loss\n",
        "classifier_criterion = nn.CrossEntropyLoss()  # Use Cross-Entropy for emotion classification loss\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)"
      ],
      "metadata": {
        "id": "RYcFptSEdxHE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "um-Jau0xNY5L"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to create a single step of the training phase\n",
        "def make_train_step(model, autoencoder_criterion, classifier_criterion, optimizer):\n",
        "\n",
        "    # define the training step of the training phase\n",
        "    def train_step(X, features, Y):\n",
        "        model.train()\n",
        "\n",
        "\n",
        "        # forward pass\n",
        "        decoded1, decoded2, output_logits, output_softmax  = model(X, features)\n",
        "        predictions = torch.argmax(output_softmax,dim=1)\n",
        "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
        "\n",
        "        # Compute losses\n",
        "        autoencoder_loss1 = autoencoder_criterion(decoded1, X)\n",
        "        autoencoder_loss2 = autoencoder_criterion(decoded2, X)\n",
        "        classifier_loss = classifier_criterion(output_logits, Y)\n",
        "\n",
        "        # Define a trade-off parameter to balance losses\n",
        "        tradeoff = 0.4 # Adjust as needed\n",
        "\n",
        "        # Calculate the combined loss\n",
        "        loss = tradeoff * (autoencoder_loss1 + autoencoder_loss2) + (1 - tradeoff) * classifier_loss\n",
        "\n",
        "        # compute gradients for the optimizer to use\n",
        "        loss.backward()\n",
        "\n",
        "        # update network parameters based on gradient stored (by calling loss.backward())\n",
        "        optimizer.step()\n",
        "\n",
        "        # zero out gradients for next pass\n",
        "        # pytorch accumulates gradients from backwards passes (convenient for RNNs)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        return loss.item(), accuracy*100, autoencoder_loss1.item(), autoencoder_loss2.item(), classifier_loss.item()\n",
        "    return train_step"
      ],
      "metadata": {
        "id": "3MQ6t3m0kaeg"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_validate_fnc(model,autoencoder_criterion, classifier_criterion):\n",
        "    def validate(X,features,Y):\n",
        "\n",
        "        # don't want to update any network parameters on validation passes: don't need gradient\n",
        "        # wrap in torch.no_grad to save memory and compute in validation phase:\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # set model to validation phase i.e. turn off dropout and batchnorm layers\n",
        "            model.eval()\n",
        "\n",
        "            # get the model's predictions on the validation set\n",
        "            decoded1, decoded2, output_logits, output_softmax  = model(X, features)\n",
        "            predictions = torch.argmax(output_softmax,dim=1)\n",
        "\n",
        "            # calculate the mean accuracy over the entire validation set\n",
        "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
        "\n",
        "            # Compute losses\n",
        "            autoencoder_loss1 = autoencoder_criterion(decoded1, X)\n",
        "            autoencoder_loss2 = autoencoder_criterion(decoded2, X)\n",
        "            classifier_loss = classifier_criterion(output_logits, Y)\n",
        "\n",
        "            # compute error from logits (nn.crossentropy implements softmax)\n",
        "            # Define a trade-off parameter to balance losses\n",
        "            tradeoff = 0.4 # Adjust as needed\n",
        "\n",
        "            # Calculate the combined loss\n",
        "            loss = tradeoff * (autoencoder_loss1 + autoencoder_loss2) + (1 - tradeoff) * classifier_loss\n",
        "\n",
        "\n",
        "        return loss.item(), accuracy*100, autoencoder_loss1.item(), autoencoder_loss2.item(), classifier_loss.item(), predictions\n",
        "    return validate"
      ],
      "metadata": {
        "id": "yLHclkxGx9vP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_save_checkpoint():\n",
        "    def save_checkpoint(optimizer, model, epoch, filename):\n",
        "        checkpoint_dict = {\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'model': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }\n",
        "        torch.save(checkpoint_dict, filename)\n",
        "    return save_checkpoint\n",
        "\n",
        "def load_checkpoint(optimizer, model, filename):\n",
        "    checkpoint_dict = torch.load(filename)\n",
        "    epoch = checkpoint_dict['epoch']\n",
        "    model.load_state_dict(checkpoint_dict['model'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    return epoch"
      ],
      "metadata": {
        "id": "qlELYNnnyUUD"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get training set size to calculate # iterations and minibatch indices\n",
        "train_size = X_train.shape[0]\n",
        "\n",
        "# pick minibatch size (of 32... always)\n",
        "minibatch = 32\n",
        "\n",
        "# set device to GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'{device} selected')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqDdx6tCLvjV",
        "outputId": "93411254-fc54-48e5-b953-9505587193cc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu selected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model and move to GPU for training\n",
        "model = MultiTaskModel(num_emotions=len(emotions_dict)).to(device)\n",
        "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwTutLftLzGm",
        "outputId": "bcb439f5-122b-4f95-a779-e652e9ce232c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable params:  259638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encountered bugs in google colab only, unless I explicitly defined optimizer in this cell...\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, weight_decay=1e-3, momentum=0.8)\n",
        "# Loss functions\n",
        "autoencoder_criterion = nn.MSELoss()  # Use Mean Squared Error for autoencoder loss\n",
        "classifier_criterion = nn.CrossEntropyLoss()  # Use Cross-Entropy for emotion classification loss"
      ],
      "metadata": {
        "id": "XIvcVjmLL2GW"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the checkpoint save function\n",
        "save_checkpoint = make_save_checkpoint()\n",
        "\n",
        "# instantiate the training step function\n",
        "train_step = make_train_step(model, autoencoder_criterion, classifier_criterion, optimizer=optimizer)\n",
        "\n",
        "# instantiate the validation loop function\n",
        "validate = make_validate_fnc(model,autoencoder_criterion, classifier_criterion)\n",
        "\n"
      ],
      "metadata": {
        "id": "VRsAfkRZL84X"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate lists to hold scalar performance metrics to plot later\n",
        "train_losses=[]\n",
        "valid_losses = []\n",
        "train_autoencoder_losses1=[]\n",
        "train_autoencoder_losses2=[]\n",
        "train_classifier_losses=[]\n",
        "\n",
        "\n",
        "valid_autoencoder_losses1=[]\n",
        "valid_autoencoder_losses2=[]\n",
        "valid_classifier_losses=[]"
      ],
      "metadata": {
        "id": "h2xNDb87YpRN"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the pretrained model and processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "w2v = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "6ec94383b035463bb23e235bcd8e65d3",
            "187e6b7d27c942da946baee44ff17ff0",
            "fb11fe42fc2f4951bcc3e6e9934627f0",
            "adcab8b859dd40a9b9fe5eff6ec443a8",
            "e9b297d89cb74addacee7ee020578d10",
            "1fe0d2af51df4005ba2cf6bca1471bfe",
            "62fba374010047c786f1e19e46ca9788",
            "b09b4b97147840408ade85b1f42a3848",
            "1536e8770a644a238287329535e44c3c",
            "5e41fc49c45b4473ba99e09843434822",
            "01baf409629a4f98916c132e260d9672",
            "d087f2eef1484eefa8094d6153b6513b",
            "b2c0e797f72343e1a95750b24188cac0",
            "808ba6525dbb4bc9afd3e93ac77cfaa0",
            "3ebac8eb13ac4385a11a4f6ea2faccbf",
            "edd884299c6c4453a1b53cf45b214d96",
            "f5c75f9993124c6a93c2158a370cb32d",
            "648998c08567411b8a558e1e9b1f2502",
            "cbfb805e8c2c4c1794717f3940493af4",
            "5fdff26e47e642b5928134be1effe4b6",
            "142cb359d38e4d3d9bec539f1e89da84",
            "83aa7f4c6ea049b18c77a46196972148",
            "3af1da0e25174c5d9e41e5db0ec134fe",
            "4ce5e2ae808e40b991dc3a5ea393b0f8",
            "bf5fea88e1f34cc28791cc20f9cb0849",
            "2e7099745f5b49e8945ff73da4df9d8d",
            "870fc667b47a4da0a79f9875382ed953",
            "fd01558caca140268e6643e382980c8e",
            "23c730c7f1ff45958e15805fa8e04ece",
            "67dd35f87f4a456b95beb14cc6a3661c",
            "896d4186250443d0a19215af97a20e2d",
            "30f505ecd95843f5b8353015690b6a09",
            "6a030c5246a648168b319b7d0663efeb",
            "2bfa266103084df78432c346bf64af25",
            "7929e7020f334c2fb8da7b3bfe25a8fd",
            "94cef12b09fc4d7293a699a5b17ea5ee",
            "0331a510e22c4c4abcb39cac680add0b",
            "94d4f608cb3340db889514af7f2151de",
            "e16b9425f62f4d98a1654ce8a5e21184",
            "0e05504aa87043ed8d67bc08af7afaed",
            "4e89f47dbd1c4c5787572efbfd6db3c2",
            "2096c14076ee4fa59ae33d6cb50d0299",
            "cdf121cec94043a88e032103b3769ba2",
            "363ff000c4c24bef8498864809835890",
            "e8bd53de97c54bbb88264ffb6be0735f",
            "51e488ed1a864fdd82089121568f35e6",
            "be8335f67d4d4ad497912da3596784c7",
            "7194f04180c840ae96d4c4dbc74ea4c2",
            "7dbadbd75fe248fcbc29ffd982ff59c4",
            "3120843b8cf8419d897eb6c167f62148",
            "c3f24fc195ce4d3b90bbabfcf25c6b94",
            "b56bacd925c8488aa7dad2bef60ddc3b",
            "1065b7938d7243daad3b9c3e6805fbab",
            "13bca219032a43b58f11367b39902747",
            "524a50ded66d4b1597451d1cb5cc938b",
            "5acbeb9f01984ca2a5a15d4bb9d1d885",
            "e0cb2c86319a48438431a4cc53b55bd6",
            "77949dc5b30e47aa84730a0c167bd2ff",
            "fc7e90c8bb0146818bdcbf38bc5b8ebd",
            "5bd7595fa4c2423da60d717af94fbcf4",
            "c8eb9cb291f7416bbc6c85f51e5428b5",
            "e71b225e47c04e538cd27659176faf9a",
            "aa22a30dd622418ab4fb80f7813c3af1",
            "08d474645d10474f90ff640f053b0cce",
            "84d6fcac58f84f4c9daa474aab67dc4e",
            "02300db6434d4eb6a6d8dad635c6bf3e"
          ]
        },
        "id": "X216zyEUzerO",
        "outputId": "619106aa-f452-40d1-89de-635b0faa6218"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)rocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ec94383b035463bb23e235bcd8e65d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d087f2eef1484eefa8094d6153b6513b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3af1da0e25174c5d9e41e5db0ec134fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bfa266103084df78432c346bf64af25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8bd53de97c54bbb88264ffb6be0735f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5acbeb9f01984ca2a5a15d4bb9d1d885"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# features_train\n",
        "# features_valid\n",
        "# features_test"
      ],
      "metadata": {
        "id": "uT0GgS9mNxI5"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create training loop for one complete epoch (entire training set)\n",
        "def train(optimizer, model, num_epochs, X_train, Features_train, Y_train, X_valid, features_valid, Y_valid):\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # set model to train phase\n",
        "        model.train()\n",
        "\n",
        "        # shuffle entire training set in each epoch to randomize minibatch order\n",
        "        train_indices = np.random.permutation(train_size)\n",
        "\n",
        "        # shuffle the training set for each epoch:\n",
        "        Features_train = Features_train[train_indices,:,:,:]\n",
        "        X_train = X_train[train_indices,:]\n",
        "        Y_train = Y_train[train_indices]\n",
        "\n",
        "\n",
        "        # instantiate scalar values to keep track of progress after each epoch so we can stop training when appropriate\n",
        "        epoch_acc = 0\n",
        "        epoch_loss = 0\n",
        "        epoch_autoencoder_loss1 = 0\n",
        "        epoch_autoencoder_loss2 = 0\n",
        "        epoch_classifier_loss = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        num_iterations = int(train_size / minibatch)\n",
        "\n",
        "        # create a loop for each minibatch of 32 samples:\n",
        "        for i in range(num_iterations):\n",
        "\n",
        "            # we have to track and update minibatch position for the current minibatch\n",
        "            # if we take a random batch position from a set, we almost certainly will skip some of the data in that set\n",
        "            # track minibatch position based on iteration number:\n",
        "            batch_start = i * minibatch\n",
        "            # ensure we don't go out of the bounds of our training set:\n",
        "            batch_end = min(batch_start + minibatch, train_size)\n",
        "            # ensure we don't have an index error\n",
        "            actual_batch_size = batch_end-batch_start\n",
        "\n",
        "            # get training minibatch with all channnels and 2D feature dims\n",
        "            features_train = Features_train[batch_start:batch_end,:,:,:]\n",
        "            # print('\\n',features_train.shape)\n",
        "\n",
        "            X = X_train[batch_start:batch_end,:]\n",
        "            # print(\"pr:\", X.shape)\n",
        "\n",
        "            audio_input = processor(X,\n",
        "                                    sampling_rate=16000,\n",
        "                                    return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "              X = w2v(**audio_input).last_hidden_state\n",
        "\n",
        "            # print(\"after:\", X.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # get training minibatch labels\n",
        "            Y = Y_train[batch_start:batch_end]\n",
        "\n",
        "            # instantiate training tensors\n",
        "            X_tensor = torch.tensor(X, device=device).float()\n",
        "            features_train = torch.tensor(features_train, device=device).float()\n",
        "\n",
        "            Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
        "            # Pass input tensors thru 1 training step (fwd+backwards pass)\n",
        "\n",
        "            loss, acc, autoencoder_loss1, autoencoder_loss2, classifier_loss = train_step(features_train, X_tensor,Y_tensor)\n",
        "\n",
        "            # aggregate batch accuracy to measure progress of entire epoch\n",
        "            epoch_acc += acc * actual_batch_size / train_size\n",
        "            epoch_loss += loss * actual_batch_size / train_size\n",
        "            epoch_autoencoder_loss1 += autoencoder_loss1 * actual_batch_size / train_size\n",
        "            epoch_autoencoder_loss2 += autoencoder_loss2 * actual_batch_size / train_size\n",
        "            epoch_classifier_loss += classifier_loss * actual_batch_size / train_size\n",
        "\n",
        "\n",
        "\n",
        "            # keep track of the iteration to see if the model's too slow\n",
        "            print('\\r'+f'Epoch {epoch}: iteration {i}/{num_iterations}',end='')\n",
        "\n",
        "        # create tensors from validation set\n",
        "        valid_audio_input = processor(X_valid,\n",
        "                                      sampling_rate=16000,\n",
        "                                      return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "          valid_audio_input_feature = w2v(**valid_audio_input).last_hidden_state\n",
        "        X_valid_tensor = torch.tensor(valid_audio_input_feature,device=device).float()\n",
        "        features_valid = torch.tensor(features_valid,device=device).float()\n",
        "        Y_valid_tensor = torch.tensor(Y_valid,dtype=torch.long,device=device)\n",
        "\n",
        "        # calculate validation metrics to keep track of progress; don't need predictions now\n",
        "        valid_loss, valid_acc, valid_autoencoder_loss1, valid_autoencoder_loss2, valid_classifier_loss, _ = validate(features_valid,\n",
        "                                                                                                                    X_valid_tensor,\n",
        "                                                                                                                     Y_valid_tensor)\n",
        "\n",
        "        # accumulate scalar performance metrics at each epoch to track and plot later\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_autoencoder_losses1.append(epoch_autoencoder_loss1)\n",
        "        train_autoencoder_losses2.append(epoch_autoencoder_loss2)\n",
        "        train_classifier_losses.append(epoch_classifier_loss)\n",
        "\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_autoencoder_losses1.append(valid_autoencoder_loss1)\n",
        "        valid_autoencoder_losses2.append(valid_autoencoder_loss2)\n",
        "        valid_classifier_losses.append(valid_classifier_loss)\n",
        "\n",
        "\n",
        "        # Save checkpoint of the model\n",
        "        checkpoint_filename = '/content/parallel_all_you_wantFINAL.pkl'.format(epoch)\n",
        "        save_checkpoint(optimizer, model, epoch, checkpoint_filename)\n",
        "\n",
        "        # keep track of each epoch's progress\n",
        "        print(f'\\nEpoch {epoch} --- loss:{epoch_loss:.3f}, Epoch accuracy:{epoch_acc:.2f}%, Validation loss:{valid_loss:.3f}, Validation accuracy:{valid_acc:.2f}%')\n",
        "        print(f'\\nEpoch {epoch} --- epoch_autoencoder_loss1:{epoch_autoencoder_loss1:.3f}, epoch_autoencoder_loss2:{epoch_autoencoder_loss2:.2f}, epoch_classifier_loss:{epoch_classifier_loss:.3f}')\n",
        "        print(f'\\nEpoch {epoch} --- valid_autoencoder_loss1:{valid_autoencoder_loss1:.3f}, valid_autoencoder_loss2:{valid_autoencoder_loss2:.2f}, valid_classifier_loss:{valid_classifier_loss:.3f}\\n\\n')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nQGzOwHCLhMs"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose number of epochs higher than reasonable so we can manually stop training\n",
        "num_epochs = 500\n",
        "\n",
        "# train it!\n",
        "train(optimizer, model, num_epochs, X_train, features_train, y_train, X_valid, features_valid, y_valid)"
      ],
      "metadata": {
        "id": "eIonIcW5Alq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abeeba19-223f-42ec-f195-e287b18c24b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: iteration 178/179\n",
            "Epoch 0 --- loss:2.174, Epoch accuracy:31.30%, Validation loss:1.813, Validation accuracy:37.62%\n",
            "\n",
            "Epoch 0 --- epoch_autoencoder_loss1:1.026, epoch_autoencoder_loss2:1.02, epoch_classifier_loss:2.256\n",
            "\n",
            "Epoch 0 --- valid_autoencoder_loss1:1.011, valid_autoencoder_loss2:1.01, valid_classifier_loss:1.676\n",
            "\n",
            "\n",
            "Epoch 1: iteration 178/179\n",
            "Epoch 1 --- loss:1.669, Epoch accuracy:43.64%, Validation loss:1.715, Validation accuracy:40.70%\n",
            "\n",
            "Epoch 1 --- epoch_autoencoder_loss1:0.965, epoch_autoencoder_loss2:0.97, epoch_classifier_loss:1.493\n",
            "\n",
            "Epoch 1 --- valid_autoencoder_loss1:0.981, valid_autoencoder_loss2:1.00, valid_classifier_loss:1.538\n",
            "\n",
            "\n",
            "Epoch 2: iteration 178/179\n",
            "Epoch 2 --- loss:1.549, Epoch accuracy:50.83%, Validation loss:1.627, Validation accuracy:51.61%\n",
            "\n",
            "Epoch 2 --- epoch_autoencoder_loss1:0.948, epoch_autoencoder_loss2:0.96, epoch_classifier_loss:1.312\n",
            "\n",
            "Epoch 2 --- valid_autoencoder_loss1:0.972, valid_autoencoder_loss2:0.98, valid_classifier_loss:1.412\n",
            "\n",
            "\n",
            "Epoch 3: iteration 178/179\n",
            "Epoch 3 --- loss:1.482, Epoch accuracy:55.33%, Validation loss:1.659, Validation accuracy:46.43%\n",
            "\n",
            "Epoch 3 --- epoch_autoencoder_loss1:0.938, epoch_autoencoder_loss2:0.95, epoch_classifier_loss:1.211\n",
            "\n",
            "Epoch 3 --- valid_autoencoder_loss1:0.965, valid_autoencoder_loss2:0.97, valid_classifier_loss:1.475\n",
            "\n",
            "\n",
            "Epoch 4: iteration 178/179\n",
            "Epoch 4 --- loss:1.414, Epoch accuracy:59.08%, Validation loss:1.546, Validation accuracy:52.59%\n",
            "\n",
            "Epoch 4 --- epoch_autoencoder_loss1:0.933, epoch_autoencoder_loss2:0.94, epoch_classifier_loss:1.106\n",
            "\n",
            "Epoch 4 --- valid_autoencoder_loss1:0.955, valid_autoencoder_loss2:0.97, valid_classifier_loss:1.297\n",
            "\n",
            "\n",
            "Epoch 5: iteration 178/179\n",
            "Epoch 5 --- loss:1.365, Epoch accuracy:62.25%, Validation loss:1.553, Validation accuracy:51.89%\n",
            "\n",
            "Epoch 5 --- epoch_autoencoder_loss1:0.929, epoch_autoencoder_loss2:0.94, epoch_classifier_loss:1.029\n",
            "\n",
            "Epoch 5 --- valid_autoencoder_loss1:0.953, valid_autoencoder_loss2:0.96, valid_classifier_loss:1.311\n",
            "\n",
            "\n",
            "Epoch 6: iteration 178/179\n",
            "Epoch 6 --- loss:1.326, Epoch accuracy:65.14%, Validation loss:1.560, Validation accuracy:54.27%\n",
            "\n",
            "Epoch 6 --- epoch_autoencoder_loss1:0.928, epoch_autoencoder_loss2:0.94, epoch_classifier_loss:0.965\n",
            "\n",
            "Epoch 6 --- valid_autoencoder_loss1:0.951, valid_autoencoder_loss2:0.96, valid_classifier_loss:1.326\n",
            "\n",
            "\n",
            "Epoch 7: iteration 178/179\n",
            "Epoch 7 --- loss:1.284, Epoch accuracy:67.22%, Validation loss:1.542, Validation accuracy:55.24%\n",
            "\n",
            "Epoch 7 --- epoch_autoencoder_loss1:0.925, epoch_autoencoder_loss2:0.94, epoch_classifier_loss:0.899\n",
            "\n",
            "Epoch 7 --- valid_autoencoder_loss1:0.948, valid_autoencoder_loss2:0.96, valid_classifier_loss:1.296\n",
            "\n",
            "\n",
            "Epoch 8: iteration 178/179\n",
            "Epoch 8 --- loss:1.267, Epoch accuracy:67.86%, Validation loss:1.487, Validation accuracy:56.64%\n",
            "\n",
            "Epoch 8 --- epoch_autoencoder_loss1:0.923, epoch_autoencoder_loss2:0.94, epoch_classifier_loss:0.871\n",
            "\n",
            "Epoch 8 --- valid_autoencoder_loss1:0.943, valid_autoencoder_loss2:0.96, valid_classifier_loss:1.212\n",
            "\n",
            "\n",
            "Epoch 9: iteration 178/179\n",
            "Epoch 9 --- loss:1.221, Epoch accuracy:71.21%, Validation loss:1.517, Validation accuracy:56.64%\n",
            "\n",
            "Epoch 9 --- epoch_autoencoder_loss1:0.922, epoch_autoencoder_loss2:0.94, epoch_classifier_loss:0.796\n",
            "\n",
            "Epoch 9 --- valid_autoencoder_loss1:0.942, valid_autoencoder_loss2:0.96, valid_classifier_loss:1.262\n",
            "\n",
            "\n",
            "Epoch 10: iteration 178/179\n",
            "Epoch 10 --- loss:1.199, Epoch accuracy:72.05%, Validation loss:1.627, Validation accuracy:53.57%\n",
            "\n",
            "Epoch 10 --- epoch_autoencoder_loss1:0.919, epoch_autoencoder_loss2:0.94, epoch_classifier_loss:0.760\n",
            "\n",
            "Epoch 10 --- valid_autoencoder_loss1:0.942, valid_autoencoder_loss2:0.96, valid_classifier_loss:1.445\n",
            "\n",
            "\n",
            "Epoch 11: iteration 178/179\n",
            "Epoch 11 --- loss:1.182, Epoch accuracy:72.62%, Validation loss:1.575, Validation accuracy:54.55%\n",
            "\n",
            "Epoch 11 --- epoch_autoencoder_loss1:0.918, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.734\n",
            "\n",
            "Epoch 11 --- valid_autoencoder_loss1:0.938, valid_autoencoder_loss2:0.95, valid_classifier_loss:1.367\n",
            "\n",
            "\n",
            "Epoch 12: iteration 178/179\n",
            "Epoch 12 --- loss:1.187, Epoch accuracy:72.75%, Validation loss:1.494, Validation accuracy:60.00%\n",
            "\n",
            "Epoch 12 --- epoch_autoencoder_loss1:0.917, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.745\n",
            "\n",
            "Epoch 12 --- valid_autoencoder_loss1:0.935, valid_autoencoder_loss2:0.95, valid_classifier_loss:1.235\n",
            "\n",
            "\n",
            "Epoch 13: iteration 178/179\n",
            "Epoch 13 --- loss:1.155, Epoch accuracy:74.75%, Validation loss:1.496, Validation accuracy:55.38%\n",
            "\n",
            "Epoch 13 --- epoch_autoencoder_loss1:0.916, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.694\n",
            "\n",
            "Epoch 13 --- valid_autoencoder_loss1:0.942, valid_autoencoder_loss2:0.95, valid_classifier_loss:1.233\n",
            "\n",
            "\n",
            "Epoch 14: iteration 178/179\n",
            "Epoch 14 --- loss:1.147, Epoch accuracy:75.54%, Validation loss:1.577, Validation accuracy:56.92%\n",
            "\n",
            "Epoch 14 --- epoch_autoencoder_loss1:0.914, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.681\n",
            "\n",
            "Epoch 14 --- valid_autoencoder_loss1:0.937, valid_autoencoder_loss2:0.95, valid_classifier_loss:1.372\n",
            "\n",
            "\n",
            "Epoch 15: iteration 178/179\n",
            "Epoch 15 --- loss:1.145, Epoch accuracy:74.37%, Validation loss:1.463, Validation accuracy:57.76%\n",
            "\n",
            "Epoch 15 --- epoch_autoencoder_loss1:0.914, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.678\n",
            "\n",
            "Epoch 15 --- valid_autoencoder_loss1:0.936, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.185\n",
            "\n",
            "\n",
            "Epoch 16: iteration 178/179\n",
            "Epoch 16 --- loss:1.147, Epoch accuracy:74.46%, Validation loss:1.577, Validation accuracy:56.08%\n",
            "\n",
            "Epoch 16 --- epoch_autoencoder_loss1:0.913, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.682\n",
            "\n",
            "Epoch 16 --- valid_autoencoder_loss1:0.937, valid_autoencoder_loss2:0.95, valid_classifier_loss:1.371\n",
            "\n",
            "\n",
            "Epoch 17: iteration 178/179\n",
            "Epoch 17 --- loss:1.092, Epoch accuracy:78.94%, Validation loss:1.550, Validation accuracy:58.88%\n",
            "\n",
            "Epoch 17 --- epoch_autoencoder_loss1:0.911, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.592\n",
            "\n",
            "Epoch 17 --- valid_autoencoder_loss1:0.941, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.327\n",
            "\n",
            "\n",
            "Epoch 18: iteration 178/179\n",
            "Epoch 18 --- loss:1.113, Epoch accuracy:76.95%, Validation loss:1.547, Validation accuracy:57.62%\n",
            "\n",
            "Epoch 18 --- epoch_autoencoder_loss1:0.911, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.628\n",
            "\n",
            "Epoch 18 --- valid_autoencoder_loss1:0.933, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.328\n",
            "\n",
            "\n",
            "Epoch 19: iteration 178/179\n",
            "Epoch 19 --- loss:1.093, Epoch accuracy:77.99%, Validation loss:1.684, Validation accuracy:56.92%\n",
            "\n",
            "Epoch 19 --- epoch_autoencoder_loss1:0.909, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.597\n",
            "\n",
            "Epoch 19 --- valid_autoencoder_loss1:0.929, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.558\n",
            "\n",
            "\n",
            "Epoch 20: iteration 178/179\n",
            "Epoch 20 --- loss:1.092, Epoch accuracy:78.36%, Validation loss:1.534, Validation accuracy:58.60%\n",
            "\n",
            "Epoch 20 --- epoch_autoencoder_loss1:0.908, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.595\n",
            "\n",
            "Epoch 20 --- valid_autoencoder_loss1:0.926, valid_autoencoder_loss2:0.95, valid_classifier_loss:1.309\n",
            "\n",
            "\n",
            "Epoch 21: iteration 178/179\n",
            "Epoch 21 --- loss:1.078, Epoch accuracy:79.34%, Validation loss:1.499, Validation accuracy:59.72%\n",
            "\n",
            "Epoch 21 --- epoch_autoencoder_loss1:0.908, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.572\n",
            "\n",
            "Epoch 21 --- valid_autoencoder_loss1:0.929, valid_autoencoder_loss2:0.95, valid_classifier_loss:1.247\n",
            "\n",
            "\n",
            "Epoch 22: iteration 178/179\n",
            "Epoch 22 --- loss:1.067, Epoch accuracy:79.90%, Validation loss:1.509, Validation accuracy:56.64%\n",
            "\n",
            "Epoch 22 --- epoch_autoencoder_loss1:0.906, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.555\n",
            "\n",
            "Epoch 22 --- valid_autoencoder_loss1:0.928, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.272\n",
            "\n",
            "\n",
            "Epoch 23: iteration 178/179\n",
            "Epoch 23 --- loss:1.066, Epoch accuracy:80.51%, Validation loss:1.588, Validation accuracy:53.01%\n",
            "\n",
            "Epoch 23 --- epoch_autoencoder_loss1:0.906, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.554\n",
            "\n",
            "Epoch 23 --- valid_autoencoder_loss1:0.928, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.401\n",
            "\n",
            "\n",
            "Epoch 24: iteration 178/179\n",
            "Epoch 24 --- loss:1.054, Epoch accuracy:80.94%, Validation loss:1.438, Validation accuracy:62.24%\n",
            "\n",
            "Epoch 24 --- epoch_autoencoder_loss1:0.905, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.535\n",
            "\n",
            "Epoch 24 --- valid_autoencoder_loss1:0.929, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.154\n",
            "\n",
            "\n",
            "Epoch 25: iteration 178/179\n",
            "Epoch 25 --- loss:1.059, Epoch accuracy:80.19%, Validation loss:1.476, Validation accuracy:58.04%\n",
            "\n",
            "Epoch 25 --- epoch_autoencoder_loss1:0.905, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.544\n",
            "\n",
            "Epoch 25 --- valid_autoencoder_loss1:0.927, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.216\n",
            "\n",
            "\n",
            "Epoch 26: iteration 178/179\n",
            "Epoch 26 --- loss:1.059, Epoch accuracy:80.09%, Validation loss:1.555, Validation accuracy:57.76%\n",
            "\n",
            "Epoch 26 --- epoch_autoencoder_loss1:0.906, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.543\n",
            "\n",
            "Epoch 26 --- valid_autoencoder_loss1:0.931, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.345\n",
            "\n",
            "\n",
            "Epoch 27: iteration 178/179\n",
            "Epoch 27 --- loss:1.059, Epoch accuracy:80.21%, Validation loss:1.648, Validation accuracy:54.13%\n",
            "\n",
            "Epoch 27 --- epoch_autoencoder_loss1:0.905, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.544\n",
            "\n",
            "Epoch 27 --- valid_autoencoder_loss1:0.929, valid_autoencoder_loss2:0.95, valid_classifier_loss:1.494\n",
            "\n",
            "\n",
            "Epoch 28: iteration 178/179\n",
            "Epoch 28 --- loss:1.049, Epoch accuracy:80.92%, Validation loss:1.709, Validation accuracy:53.85%\n",
            "\n",
            "Epoch 28 --- epoch_autoencoder_loss1:0.904, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.529\n",
            "\n",
            "Epoch 28 --- valid_autoencoder_loss1:0.928, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.605\n",
            "\n",
            "\n",
            "Epoch 29: iteration 178/179\n",
            "Epoch 29 --- loss:1.044, Epoch accuracy:81.24%, Validation loss:1.543, Validation accuracy:57.20%\n",
            "\n",
            "Epoch 29 --- epoch_autoencoder_loss1:0.904, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.520\n",
            "\n",
            "Epoch 29 --- valid_autoencoder_loss1:0.926, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.324\n",
            "\n",
            "\n",
            "Epoch 30: iteration 178/179\n",
            "Epoch 30 --- loss:1.040, Epoch accuracy:82.09%, Validation loss:1.476, Validation accuracy:62.52%\n",
            "\n",
            "Epoch 30 --- epoch_autoencoder_loss1:0.903, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.514\n",
            "\n",
            "Epoch 30 --- valid_autoencoder_loss1:0.924, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.221\n",
            "\n",
            "\n",
            "Epoch 31: iteration 178/179\n",
            "Epoch 31 --- loss:1.028, Epoch accuracy:82.28%, Validation loss:1.528, Validation accuracy:59.02%\n",
            "\n",
            "Epoch 31 --- epoch_autoencoder_loss1:0.903, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.494\n",
            "\n",
            "Epoch 31 --- valid_autoencoder_loss1:0.926, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.302\n",
            "\n",
            "\n",
            "Epoch 32: iteration 178/179\n",
            "Epoch 32 --- loss:1.045, Epoch accuracy:80.92%, Validation loss:1.433, Validation accuracy:62.24%\n",
            "\n",
            "Epoch 32 --- epoch_autoencoder_loss1:0.905, epoch_autoencoder_loss2:0.92, epoch_classifier_loss:0.523\n",
            "\n",
            "Epoch 32 --- valid_autoencoder_loss1:0.925, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.146\n",
            "\n",
            "\n",
            "Epoch 33: iteration 178/179\n",
            "Epoch 33 --- loss:1.037, Epoch accuracy:81.87%, Validation loss:1.494, Validation accuracy:62.66%\n",
            "\n",
            "Epoch 33 --- epoch_autoencoder_loss1:0.903, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.508\n",
            "\n",
            "Epoch 33 --- valid_autoencoder_loss1:0.928, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.244\n",
            "\n",
            "\n",
            "Epoch 34: iteration 178/179\n",
            "Epoch 34 --- loss:1.046, Epoch accuracy:81.39%, Validation loss:1.552, Validation accuracy:60.56%\n",
            "\n",
            "Epoch 34 --- epoch_autoencoder_loss1:0.903, epoch_autoencoder_loss2:0.93, epoch_classifier_loss:0.523\n",
            "\n",
            "Epoch 34 --- valid_autoencoder_loss1:0.923, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.345\n",
            "\n",
            "\n",
            "Epoch 35: iteration 178/179\n",
            "Epoch 35 --- loss:1.025, Epoch accuracy:82.67%, Validation loss:1.500, Validation accuracy:60.42%\n",
            "\n",
            "Epoch 35 --- epoch_autoencoder_loss1:0.902, epoch_autoencoder_loss2:0.92, epoch_classifier_loss:0.490\n",
            "\n",
            "Epoch 35 --- valid_autoencoder_loss1:0.926, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.258\n",
            "\n",
            "\n",
            "Epoch 36: iteration 178/179\n",
            "Epoch 36 --- loss:1.031, Epoch accuracy:82.11%, Validation loss:1.505, Validation accuracy:65.59%\n",
            "\n",
            "Epoch 36 --- epoch_autoencoder_loss1:0.903, epoch_autoencoder_loss2:0.92, epoch_classifier_loss:0.500\n",
            "\n",
            "Epoch 36 --- valid_autoencoder_loss1:0.927, valid_autoencoder_loss2:0.94, valid_classifier_loss:1.263\n",
            "\n",
            "\n",
            "Epoch 37: iteration 74/179"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Loss Curve for Parallel is All You Want Model')\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.xlabel('Epoch', fontsize=16)\n",
        "plt.plot(train_losses[:],'b')\n",
        "plt.plot(valid_losses[:],'r')\n",
        "plt.legend(['Training loss','Validation loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EqPuvmdpDCm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0GvCUtUkygoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lgy56SzqcNUE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "KERNEL",
      "language": "python",
      "name": "kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ec94383b035463bb23e235bcd8e65d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_187e6b7d27c942da946baee44ff17ff0",
              "IPY_MODEL_fb11fe42fc2f4951bcc3e6e9934627f0",
              "IPY_MODEL_adcab8b859dd40a9b9fe5eff6ec443a8"
            ],
            "layout": "IPY_MODEL_e9b297d89cb74addacee7ee020578d10"
          }
        },
        "187e6b7d27c942da946baee44ff17ff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fe0d2af51df4005ba2cf6bca1471bfe",
            "placeholder": "​",
            "style": "IPY_MODEL_62fba374010047c786f1e19e46ca9788",
            "value": "Downloading (…)rocessor_config.json: 100%"
          }
        },
        "fb11fe42fc2f4951bcc3e6e9934627f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b09b4b97147840408ade85b1f42a3848",
            "max": 159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1536e8770a644a238287329535e44c3c",
            "value": 159
          }
        },
        "adcab8b859dd40a9b9fe5eff6ec443a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e41fc49c45b4473ba99e09843434822",
            "placeholder": "​",
            "style": "IPY_MODEL_01baf409629a4f98916c132e260d9672",
            "value": " 159/159 [00:00&lt;00:00, 13.9kB/s]"
          }
        },
        "e9b297d89cb74addacee7ee020578d10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fe0d2af51df4005ba2cf6bca1471bfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62fba374010047c786f1e19e46ca9788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b09b4b97147840408ade85b1f42a3848": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1536e8770a644a238287329535e44c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e41fc49c45b4473ba99e09843434822": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01baf409629a4f98916c132e260d9672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d087f2eef1484eefa8094d6153b6513b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2c0e797f72343e1a95750b24188cac0",
              "IPY_MODEL_808ba6525dbb4bc9afd3e93ac77cfaa0",
              "IPY_MODEL_3ebac8eb13ac4385a11a4f6ea2faccbf"
            ],
            "layout": "IPY_MODEL_edd884299c6c4453a1b53cf45b214d96"
          }
        },
        "b2c0e797f72343e1a95750b24188cac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5c75f9993124c6a93c2158a370cb32d",
            "placeholder": "​",
            "style": "IPY_MODEL_648998c08567411b8a558e1e9b1f2502",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "808ba6525dbb4bc9afd3e93ac77cfaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbfb805e8c2c4c1794717f3940493af4",
            "max": 163,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fdff26e47e642b5928134be1effe4b6",
            "value": 163
          }
        },
        "3ebac8eb13ac4385a11a4f6ea2faccbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_142cb359d38e4d3d9bec539f1e89da84",
            "placeholder": "​",
            "style": "IPY_MODEL_83aa7f4c6ea049b18c77a46196972148",
            "value": " 163/163 [00:00&lt;00:00, 13.3kB/s]"
          }
        },
        "edd884299c6c4453a1b53cf45b214d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5c75f9993124c6a93c2158a370cb32d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "648998c08567411b8a558e1e9b1f2502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbfb805e8c2c4c1794717f3940493af4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fdff26e47e642b5928134be1effe4b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "142cb359d38e4d3d9bec539f1e89da84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83aa7f4c6ea049b18c77a46196972148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3af1da0e25174c5d9e41e5db0ec134fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ce5e2ae808e40b991dc3a5ea393b0f8",
              "IPY_MODEL_bf5fea88e1f34cc28791cc20f9cb0849",
              "IPY_MODEL_2e7099745f5b49e8945ff73da4df9d8d"
            ],
            "layout": "IPY_MODEL_870fc667b47a4da0a79f9875382ed953"
          }
        },
        "4ce5e2ae808e40b991dc3a5ea393b0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd01558caca140268e6643e382980c8e",
            "placeholder": "​",
            "style": "IPY_MODEL_23c730c7f1ff45958e15805fa8e04ece",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "bf5fea88e1f34cc28791cc20f9cb0849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67dd35f87f4a456b95beb14cc6a3661c",
            "max": 1596,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_896d4186250443d0a19215af97a20e2d",
            "value": 1596
          }
        },
        "2e7099745f5b49e8945ff73da4df9d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30f505ecd95843f5b8353015690b6a09",
            "placeholder": "​",
            "style": "IPY_MODEL_6a030c5246a648168b319b7d0663efeb",
            "value": " 1.60k/1.60k [00:00&lt;00:00, 99.1kB/s]"
          }
        },
        "870fc667b47a4da0a79f9875382ed953": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd01558caca140268e6643e382980c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c730c7f1ff45958e15805fa8e04ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67dd35f87f4a456b95beb14cc6a3661c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "896d4186250443d0a19215af97a20e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30f505ecd95843f5b8353015690b6a09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a030c5246a648168b319b7d0663efeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bfa266103084df78432c346bf64af25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7929e7020f334c2fb8da7b3bfe25a8fd",
              "IPY_MODEL_94cef12b09fc4d7293a699a5b17ea5ee",
              "IPY_MODEL_0331a510e22c4c4abcb39cac680add0b"
            ],
            "layout": "IPY_MODEL_94d4f608cb3340db889514af7f2151de"
          }
        },
        "7929e7020f334c2fb8da7b3bfe25a8fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e16b9425f62f4d98a1654ce8a5e21184",
            "placeholder": "​",
            "style": "IPY_MODEL_0e05504aa87043ed8d67bc08af7afaed",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "94cef12b09fc4d7293a699a5b17ea5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e89f47dbd1c4c5787572efbfd6db3c2",
            "max": 291,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2096c14076ee4fa59ae33d6cb50d0299",
            "value": 291
          }
        },
        "0331a510e22c4c4abcb39cac680add0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdf121cec94043a88e032103b3769ba2",
            "placeholder": "​",
            "style": "IPY_MODEL_363ff000c4c24bef8498864809835890",
            "value": " 291/291 [00:00&lt;00:00, 24.6kB/s]"
          }
        },
        "94d4f608cb3340db889514af7f2151de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e16b9425f62f4d98a1654ce8a5e21184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e05504aa87043ed8d67bc08af7afaed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e89f47dbd1c4c5787572efbfd6db3c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2096c14076ee4fa59ae33d6cb50d0299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdf121cec94043a88e032103b3769ba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "363ff000c4c24bef8498864809835890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8bd53de97c54bbb88264ffb6be0735f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51e488ed1a864fdd82089121568f35e6",
              "IPY_MODEL_be8335f67d4d4ad497912da3596784c7",
              "IPY_MODEL_7194f04180c840ae96d4c4dbc74ea4c2"
            ],
            "layout": "IPY_MODEL_7dbadbd75fe248fcbc29ffd982ff59c4"
          }
        },
        "51e488ed1a864fdd82089121568f35e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3120843b8cf8419d897eb6c167f62148",
            "placeholder": "​",
            "style": "IPY_MODEL_c3f24fc195ce4d3b90bbabfcf25c6b94",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "be8335f67d4d4ad497912da3596784c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b56bacd925c8488aa7dad2bef60ddc3b",
            "max": 85,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1065b7938d7243daad3b9c3e6805fbab",
            "value": 85
          }
        },
        "7194f04180c840ae96d4c4dbc74ea4c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13bca219032a43b58f11367b39902747",
            "placeholder": "​",
            "style": "IPY_MODEL_524a50ded66d4b1597451d1cb5cc938b",
            "value": " 85.0/85.0 [00:00&lt;00:00, 8.11kB/s]"
          }
        },
        "7dbadbd75fe248fcbc29ffd982ff59c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3120843b8cf8419d897eb6c167f62148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3f24fc195ce4d3b90bbabfcf25c6b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b56bacd925c8488aa7dad2bef60ddc3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1065b7938d7243daad3b9c3e6805fbab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13bca219032a43b58f11367b39902747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "524a50ded66d4b1597451d1cb5cc938b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5acbeb9f01984ca2a5a15d4bb9d1d885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0cb2c86319a48438431a4cc53b55bd6",
              "IPY_MODEL_77949dc5b30e47aa84730a0c167bd2ff",
              "IPY_MODEL_fc7e90c8bb0146818bdcbf38bc5b8ebd"
            ],
            "layout": "IPY_MODEL_5bd7595fa4c2423da60d717af94fbcf4"
          }
        },
        "e0cb2c86319a48438431a4cc53b55bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8eb9cb291f7416bbc6c85f51e5428b5",
            "placeholder": "​",
            "style": "IPY_MODEL_e71b225e47c04e538cd27659176faf9a",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "77949dc5b30e47aa84730a0c167bd2ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa22a30dd622418ab4fb80f7813c3af1",
            "max": 377607901,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08d474645d10474f90ff640f053b0cce",
            "value": 377607901
          }
        },
        "fc7e90c8bb0146818bdcbf38bc5b8ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d6fcac58f84f4c9daa474aab67dc4e",
            "placeholder": "​",
            "style": "IPY_MODEL_02300db6434d4eb6a6d8dad635c6bf3e",
            "value": " 378M/378M [00:00&lt;00:00, 526MB/s]"
          }
        },
        "5bd7595fa4c2423da60d717af94fbcf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8eb9cb291f7416bbc6c85f51e5428b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e71b225e47c04e538cd27659176faf9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa22a30dd622418ab4fb80f7813c3af1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08d474645d10474f90ff640f053b0cce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84d6fcac58f84f4c9daa474aab67dc4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02300db6434d4eb6a6d8dad635c6bf3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}